{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST TensorFlow\n",
    "We use:\n",
    "* convolutional and fully-connected layers\n",
    "* regularization techniques:\n",
    "    * dropout\n",
    "    * max pulling (to help overfitting by providing an abstracted form of the representation + it reduces the computational cost by reducing the number of parameters)\n",
    "* activation functions:\n",
    "    * ReLU\n",
    "    * softmax for probabilities (classification)\n",
    "* flexible learning rate\n",
    "* batch normalization\n",
    "    * batch norm scaling is not useful with relus\n",
    "    * batch norm offsets are used instead of biases\n",
    "* TensorBoard vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    d = datetime.datetime.now()\n",
    "    return d.strftime(\"%Y/%m/%d/%X\")\n",
    "\n",
    "logs_path_train = '/tmp/tensorflow_logs/mnist/train' + timestamp()\n",
    "logs_path_val = '/tmp/tensorflow_logs/mnist/val' + timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "max_learning_rate = 0.0004\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 10\n",
    "bnepsilon = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"x\")\n",
    "tf.summary.image('input', x)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"labels\")\n",
    "# Learning rate\n",
    "lr = tf.placeholder(tf.float32,)\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout) \n",
    "# and 0.75 at training time\n",
    "pkeep_fc = tf.placeholder(tf.float32)\n",
    "pkeep_conv = tf.placeholder(tf.float32)\n",
    "# test flag for batch norm\n",
    "tst = tf.placeholder(tf.bool)\n",
    "iter = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    # adding the iteration prevents from averaging across non-existing iterations\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) \n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "def compatible_convolutional_noise_shape(Y):\n",
    "    noiseshape = tf.shape(Y)\n",
    "    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
    "    return noiseshape\n",
    "\n",
    "def conv_layer(input, size_window, size_in, size_out, stride, max_pulling=True, name=\"conv\"):\n",
    "    \"\"\"Convolutional layer + max pulling + dropout.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(\n",
    "            tf.truncated_normal([size_window, size_window, size_in, size_out], stddev=0.1), name=\"weight\")\n",
    "        b = tf.Variable(tf.ones([size_out]), name=\"bias\")\n",
    "        # CNN\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        # BATCH NORMALIZATION\n",
    "        act, update_ema = batchnorm(conv, tst, iter, b, convolutional=True)\n",
    "        # ReLU activation\n",
    "        act = tf.nn.relu(conv)\n",
    "        if max_pulling == True:\n",
    "            # MAX PULLING\n",
    "            act = tf.nn.max_pool(\n",
    "                act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            # DROPOUT\n",
    "            act = tf.nn.dropout(\n",
    "                act, pkeep_conv, compatible_convolutional_noise_shape(act))\n",
    "        else:\n",
    "            # DROPOUT\n",
    "            act = tf.nn.dropout(\n",
    "                act, pkeep_conv, compatible_convolutional_noise_shape(act))\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act, update_ema\n",
    "    \n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    \"\"\"Fully-connected layer.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), \n",
    "                        name=\"weight\")\n",
    "        b = tf.Variable(tf.ones([size_out]), name=\"bias\")\n",
    "        # FC logits\n",
    "        fc = tf.matmul(input, w)\n",
    "        # BATCH NORMALIZATION\n",
    "        act, update_ema = batchnorm(fc, tst, iter, b, convolutional=False)\n",
    "        # ReLU activation\n",
    "        act = tf.nn.relu(act)\n",
    "\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act, update_ema\n",
    "    \n",
    "def get_logits(input, size_in, size_out, name=\"get_logits\"):\n",
    "    \"\"\"Fully-connected layer without activation function.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), \n",
    "                        name=\"weight\")\n",
    "        b = tf.Variable(tf.ones([size_out]), name=\"bias\")\n",
    "        logits = tf.matmul(input, w) + b\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# input is 28x28\n",
    "conv1, update_ema1 = conv_layer(x, 6, 1, 24, 1, max_pulling=True, name='conv1')\n",
    "# stride = 1 and max pooling -> output is size_out (24) feature maps with 14x14 size\n",
    "conv2, update_ema2 = conv_layer(conv1, 5, 24, 48, 1, max_pulling=True, name='conv2')\n",
    "# stride = 1 (\"same\" padding -> size remains constant) and max pooling -> size changed: output is 7x7\n",
    "conv3, update_ema3 = conv_layer(conv2, 4, 48, 64, 1, max_pulling=False, name='conv3')\n",
    "# stride = 1 -> output is 7x7\n",
    "flattened = tf.reshape(conv3, shape=[-1, 7 * 7 * 64])\n",
    "fc1, update_ema4 = fc_layer(flattened, 7 * 7 * 64, 200, \"fc1\")\n",
    "logits = get_logits(fc1, 200, 10, \"logits\")\n",
    "\n",
    "update_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_), name='loss')\n",
    "    tf.summary.scalar(\"loss\", cross_entropy)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    tf.summary.scalar(\"accuracy\", acc)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train loss 0.383475291 val loss 0.246662935 learning rate 0.000400000\n",
      "Epoch: 0002 train loss 0.119240872 val loss 0.074134740 learning rate 0.000371451\n",
      "Epoch: 0003 train loss 0.085277008 val loss 0.054309076 learning rate 0.000345619\n",
      "Epoch: 0004 train loss 0.068126769 val loss 0.044543734 learning rate 0.000322245\n",
      "Epoch: 0005 train loss 0.056819013 val loss 0.039922873 learning rate 0.000301096\n",
      "Epoch: 0006 train loss 0.049651470 val loss 0.037398321 learning rate 0.000281959\n",
      "Epoch: 0007 train loss 0.044833362 val loss 0.034619518 learning rate 0.000264643\n",
      "Epoch: 0008 train loss 0.040875166 val loss 0.032598043 learning rate 0.000248976\n",
      "Epoch: 0009 train loss 0.039633398 val loss 0.031267214 learning rate 0.000234799\n",
      "Epoch: 0010 train loss 0.036536123 val loss 0.030053893 learning rate 0.000221971\n",
      "Epoch: 0011 train loss 0.032441446 val loss 0.028197201 learning rate 0.000210364\n",
      "Epoch: 0012 train loss 0.030286548 val loss 0.026890954 learning rate 0.000199861\n",
      "Epoch: 0013 train loss 0.029201461 val loss 0.026797135 learning rate 0.000190358\n",
      "Epoch: 0014 train loss 0.026256326 val loss 0.025285554 learning rate 0.000181760\n",
      "Epoch: 0015 train loss 0.026470033 val loss 0.025768991 learning rate 0.000173979\n",
      "Epoch: 0016 train loss 0.023626750 val loss 0.025408972 learning rate 0.000166939\n",
      "Epoch: 0017 train loss 0.024089639 val loss 0.025099197 learning rate 0.000160569\n",
      "Epoch: 0018 train loss 0.021486209 val loss 0.024932592 learning rate 0.000154805\n",
      "Epoch: 0019 train loss 0.019914417 val loss 0.024082999 learning rate 0.000149590\n",
      "Epoch: 0020 train loss 0.020691418 val loss 0.023207146 learning rate 0.000144871\n",
      "Optimization Finished!\n",
      "total_test_batch 100\n",
      "Accuracy: 0.9944\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/\n",
      "CPU times: user 3h 3min 34s, sys: 16min 11s, total: 3h 19min 45s\n",
      "Wall time: 58min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer_train = tf.summary.FileWriter(logs_path_train, \n",
    "                                                 graph=tf.get_default_graph())\n",
    "    summary_writer_val = tf.summary.FileWriter(logs_path_val, \n",
    "                                               graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_train_loss = 0.\n",
    "        avg_val_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        learning_rate = min_learning_rate + (\n",
    "            max_learning_rate - min_learning_rate) * np.exp(-epoch / decay_speed)\n",
    "          \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x_train, batch_y_train = mnist.train.next_batch(batch_size)\n",
    "            batch_x_val, batch_y_val = mnist.validation.next_batch(batch_size)\n",
    "            \n",
    "            # Run optimization op (backprop), cost op (to get loss value) and summary nodes\n",
    "            # and summary nodes\n",
    "            a, c, summary = sess.run([optimizer, cross_entropy, merged_summary_op], \n",
    "                                     feed_dict={x: batch_x_train, \n",
    "                                                y_: batch_y_train, \n",
    "                                                lr: learning_rate,\n",
    "                                                tst: False, \n",
    "                                                pkeep_fc: 0.75, \n",
    "                                                pkeep_conv: 0.75}) \n",
    "            \n",
    "            sess.run(update_ema, {x: batch_x_train, \n",
    "                                  y_: batch_y_train, \n",
    "                                  tst: False, \n",
    "                                  iter: i, \n",
    "                                  pkeep_fc: 0.75, \n",
    "                                  pkeep_conv: 0.75})\n",
    "            \n",
    "            c_val, summary_val = sess.run([cross_entropy, merged_summary_op], \n",
    "                                          feed_dict={x: batch_x_val, \n",
    "                                                     y_: batch_y_val, \n",
    "                                                     lr: learning_rate, \n",
    "                                                     tst: False, \n",
    "                                                     pkeep_fc: 1.0, \n",
    "                                                     pkeep_conv: 1.0}) \n",
    "\n",
    "            # Write logs at every iteration\n",
    "            summary_writer_train.add_summary(summary, epoch * total_batch + i)\n",
    "            summary_writer_val.add_summary(summary_val, epoch * total_batch + i)\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_train_loss += c / total_batch\n",
    "            avg_val_loss += c_val / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "                  \"train loss\", \"{:.9f}\".format(avg_train_loss),\n",
    "                  \"val loss\", \"{:.9f}\".format(avg_val_loss),\n",
    "                  \"learning rate\", \"{:.9f}\".format(learning_rate))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy    \n",
    "    total_test_batch = mnist.test.num_examples // batch_size\n",
    "    print('total_test_batch', total_test_batch)    \n",
    "    acc_test_lst = []\n",
    "    for step in range(mnist.test.images.shape[0] // total_test_batch):        \n",
    "        acc_test = acc.eval(\n",
    "            {x: mnist.test.images[step * total_test_batch:(step + 1) * total_test_batch, :], \n",
    "             y_: mnist.test.labels[step * total_test_batch:(step + 1) * total_test_batch, :],\n",
    "             lr: learning_rate,\n",
    "             tst: False, \n",
    "             pkeep_fc: 1.0, \n",
    "             pkeep_conv: 1.0}) \n",
    "        acc_test_lst.append(acc_test)    \n",
    "    print(\"Accuracy:\", np.mean(acc_test_lst))\n",
    "\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
