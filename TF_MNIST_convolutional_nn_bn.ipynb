{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def timestamp():\n",
    "    d = datetime.datetime.now()\n",
    "    return d.strftime(\"%Y/%m/%d/%X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "max_learning_rate = 0.0004\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 10\n",
    "bnepsilon = 1e-5\n",
    "\n",
    "logs_path_train = '/tmp/tensorflow_logs/example/train' + timestamp()\n",
    "logs_path_val = '/tmp/tensorflow_logs/example/val' + timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# variable learning rate\n",
    "lr = tf.placeholder(tf.float32)\n",
    "# test flag for batch norm\n",
    "tst = tf.placeholder(tf.bool)\n",
    "iter = tf.placeholder(tf.int32)\n",
    "# Probability of keeping a node during dropout = 1.0 at test time (no dropout)  and 0.75 at training time\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "pkeep_conv = tf.placeholder(tf.float32)\n",
    "\n",
    "# Variables\n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, 24], stddev=0.1))\n",
    "b1 = tf.Variable(tf.ones([24]))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 24, 48], stddev=0.1))\n",
    "b2 = tf.Variable(tf.ones([48]))\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, 48, 64], stddev=0.1))\n",
    "b3 = tf.Variable(tf.ones([64]))\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 200], stddev=0.1))\n",
    "b4 = tf.Variable(tf.ones([200]))\n",
    "W5 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.ones([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    # adding the iteration prevents from averaging across non-existing iterations\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) \n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "def compatible_convolutional_noise_shape(Y):\n",
    "    noiseshape = tf.shape(Y)\n",
    "    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n",
    "    return noiseshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The model\n",
    "# batch norm scaling is not useful with relus\n",
    "# batch norm offsets are used instead of biases\n",
    "\n",
    "stride = 1  # output is 28x28\n",
    "y1l = tf.nn.conv2d(x, W1, strides=[1, stride, stride, 1], padding='SAME')\n",
    "y1bn, update_ema1 = batchnorm(y1l, tst, iter, b1, convolutional=True)\n",
    "y1r = tf.nn.relu(y1bn)\n",
    "y1 = tf.nn.dropout(y1r, pkeep_conv, compatible_convolutional_noise_shape(y1r))\n",
    "\n",
    "stride = 2  # output is 14x14\n",
    "y2l = tf.nn.conv2d(y1, W2, strides=[1, stride, stride, 1], padding='SAME')\n",
    "y2bn, update_ema2 = batchnorm(y2l, tst, iter, b2, convolutional=True)\n",
    "y2r = tf.nn.relu(y2bn)\n",
    "y2 = tf.nn.dropout(y2r, pkeep_conv, compatible_convolutional_noise_shape(y2r))\n",
    "\n",
    "stride = 2  # output is 7x7\n",
    "y3l = tf.nn.conv2d(y2, W3, strides=[1, stride, stride, 1], padding='SAME')\n",
    "y3bn, update_ema3 = batchnorm(y3l, tst, iter, b3, convolutional=True)\n",
    "y3r = tf.nn.relu(y3bn)\n",
    "y3 = tf.nn.dropout(y3r, pkeep_conv, compatible_convolutional_noise_shape(y3r))\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "y3_conv = tf.reshape(y3, shape=[-1, 7 * 7 * 64])\n",
    "\n",
    "y4l = tf.matmul(y3_conv, W4)\n",
    "y4bn, update_ema4 = batchnorm(y4l, tst, iter, b4)\n",
    "y4r = tf.nn.relu(y4bn)\n",
    "y4 = tf.nn.dropout(y4r, pkeep)\n",
    "y = tf.matmul(y4, W5) + b5\n",
    "\n",
    "update_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "\n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cross_entropy)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train loss 0.679447803 val loss 0.429449026 learning rate 0.000400000\n",
      "Epoch: 0002 train loss 0.296709327 val loss 0.187034261 learning rate 0.000371451\n",
      "Epoch: 0003 train loss 0.205166025 val loss 0.127128779 learning rate 0.000345619\n",
      "Epoch: 0004 train loss 0.159735332 val loss 0.100147673 learning rate 0.000322245\n",
      "Epoch: 0005 train loss 0.133731233 val loss 0.083816353 learning rate 0.000301096\n",
      "Epoch: 0006 train loss 0.115231307 val loss 0.072443589 learning rate 0.000281959\n",
      "Epoch: 0007 train loss 0.103943510 val loss 0.065619865 learning rate 0.000264643\n",
      "Epoch: 0008 train loss 0.095510741 val loss 0.061461575 learning rate 0.000248976\n",
      "Epoch: 0009 train loss 0.086480129 val loss 0.056793766 learning rate 0.000234799\n",
      "Epoch: 0010 train loss 0.081258942 val loss 0.052968601 learning rate 0.000221971\n",
      "Epoch: 0011 train loss 0.074817300 val loss 0.049768013 learning rate 0.000210364\n",
      "Epoch: 0012 train loss 0.070222282 val loss 0.047685462 learning rate 0.000199861\n",
      "Epoch: 0013 train loss 0.065794739 val loss 0.046494180 learning rate 0.000190358\n",
      "Epoch: 0014 train loss 0.060935289 val loss 0.045147519 learning rate 0.000181760\n",
      "Epoch: 0015 train loss 0.059513359 val loss 0.043721317 learning rate 0.000173979\n",
      "Epoch: 0016 train loss 0.056300392 val loss 0.042367279 learning rate 0.000166939\n",
      "Epoch: 0017 train loss 0.053925263 val loss 0.041610913 learning rate 0.000160569\n",
      "Epoch: 0018 train loss 0.051969874 val loss 0.040359950 learning rate 0.000154805\n",
      "Epoch: 0019 train loss 0.050380956 val loss 0.040253041 learning rate 0.000149590\n",
      "Epoch: 0020 train loss 0.050622180 val loss 0.039196820 learning rate 0.000144871\n",
      "Epoch: 0021 train loss 0.047676822 val loss 0.038354772 learning rate 0.000140601\n",
      "Epoch: 0022 train loss 0.046094525 val loss 0.037727964 learning rate 0.000136737\n",
      "Epoch: 0023 train loss 0.046634853 val loss 0.037858190 learning rate 0.000133241\n",
      "Epoch: 0024 train loss 0.044382100 val loss 0.036920933 learning rate 0.000130078\n",
      "Epoch: 0025 train loss 0.042392063 val loss 0.036169072 learning rate 0.000127215\n",
      "Optimization Finished!\n",
      "total_test_batch 100\n",
      "Accuracy: 0.991\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer_train = tf.summary.FileWriter(logs_path_train, \n",
    "                                                 graph=tf.get_default_graph())\n",
    "    summary_writer_val = tf.summary.FileWriter(logs_path_val, \n",
    "                                               graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_train_loss = 0.\n",
    "        avg_val_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        learning_rate = min_learning_rate + (\n",
    "            max_learning_rate - min_learning_rate) * math.exp(-epoch / decay_speed)  \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x_train, batch_y_train = mnist.train.next_batch(batch_size)\n",
    "            batch_x_val, batch_y_val = mnist.validation.next_batch(batch_size)\n",
    "            \n",
    "            # Run optimization op (backprop), cost op (to get loss value) and summary nodes\n",
    "            # and summary nodes\n",
    "            a, c, summary = sess.run([optimizer, cross_entropy, merged_summary_op], \n",
    "                                     feed_dict={x: batch_x_train, \n",
    "                                                y_: batch_y_train,\n",
    "                                                lr: learning_rate, \n",
    "                                                tst: False,\n",
    "                                                pkeep: 0.75, \n",
    "                                                pkeep_conv: 0.75})  \n",
    "            \n",
    "            sess.run(update_ema, {x: batch_x_train, \n",
    "                                  y_: batch_y_train, \n",
    "                                  tst: False, \n",
    "                                  iter: i, \n",
    "                                  pkeep: 0.75, \n",
    "                                  pkeep_conv: 0.75})\n",
    "           \n",
    "            c_val, summary_val = sess.run([cross_entropy, merged_summary_op], \n",
    "                                          feed_dict={x: batch_x_val, \n",
    "                                                     y_: batch_y_val,\n",
    "                                                     lr: learning_rate, \n",
    "                                                     tst: False, \n",
    "                                                     pkeep: 1.0, \n",
    "                                                     pkeep_conv: 1.0}) \n",
    "            \n",
    "            \n",
    "            # Write logs at every iteration\n",
    "            summary_writer_train.add_summary(summary, epoch * total_batch + i)\n",
    "            summary_writer_val.add_summary(summary_val, epoch * total_batch + i)\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_train_loss += c / total_batch\n",
    "            avg_val_loss += c_val / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "                  \"train loss\", \"{:.9f}\".format(avg_train_loss),\n",
    "                  \"val loss\", \"{:.9f}\".format(avg_val_loss),\n",
    "                  \"learning rate\", \"{:.9f}\".format(learning_rate))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy    \n",
    "    total_test_batch = mnist.test.num_examples // batch_size\n",
    "    print('total_test_batch', total_test_batch)    \n",
    "    acc_test_lst = []\n",
    "    for step in range(mnist.test.images.shape[0] // total_test_batch):        \n",
    "        acc_test = acc.eval(\n",
    "            {x: mnist.test.images[step * total_test_batch:(step + 1) * total_test_batch, :], \n",
    "             y_: mnist.test.labels[step * total_test_batch:(step + 1) * total_test_batch, :],\n",
    "             lr: learning_rate, \n",
    "             tst: False, \n",
    "             pkeep: 1.0, \n",
    "             pkeep_conv: 1.0}) \n",
    "        acc_test_lst.append(acc_test)    \n",
    "    print(\"Accuracy:\", np.mean(acc_test_lst))\n",
    "\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
